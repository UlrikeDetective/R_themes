rename(ID = ImageName)
Finnland2025
# Add the prefix 'IPhone_Q4_' to each entry in the ID column
Finnland2025$ID <- paste0("IPhone_Finnland2025", Finnland2025$ID)
# Add the prefix 'IPhone_Q4_' to each entry in the ID column
Finnland2025$ID <- paste0("IPhone_Finnland2025", Finnland2025$ID)
Finnland2025
library(readxl)
Finnland2025 <- read.csv("/Users/ulrike_imac_air/projects/analysis_my_life/data/photos_gps/2025/2025_Finnland.csv")
# Remove rows where GPSData is NA or empty
Finnland2025 <- Finnland2025[!(is.na(Finnland2025$GPSData) | Finnland2025$GPSData == ""), ]
Finnland2025
# Changing the "Date" from chr to date format
library(dplyr)
Finnland2025 <- Finnland2025 %>%
mutate(Date = as.Date(Date, format = "%Y:%m:%d"))
head(Finnland2025)
# Load necessary library if not already loaded
library(dplyr)
library(chron)
# Assuming your dataframe is named Finnland2025
Finnland2025 <- Finnland2025 %>%
mutate(Time = times(Time))
head(Finnland2025)
# Extracting the first part of GPSData and putting it in a new column called lat_dms
library(dplyr)
library(stringr)
Finnland2025 <- Finnland2025 %>%
mutate(lat_dms = str_extract(GPSData, "^[^,]+"))
# Extracting the second part of GPSData and putting it in a new column called lng_dms
Finnland2025 <- Finnland2025 %>%
mutate(lng_dms = str_extract(GPSData, "(?<=, ).*(?=, )"))
# Extracting the last part of GPSData and putting it in a new column called altitude. And changing data format from chr to integer. Also round values.
Finnland2025 <- Finnland2025 %>%
mutate(Altitude = round(as.numeric(str_extract(GPSData, "(?<=Altitude: )[^,]+")), 2))
# Converting DMS to decimal degree format. Starting with the latitude
Finnland2025 <- Finnland2025 %>%
mutate(lat_decimaldegrees = {
# Extract degrees, minutes, seconds, and hemisphere
lat_parts <- str_match(lat_dms, "([0-9.]+)°\\s([0-9.]+)'\\s([0-9.]+)''\\s([NS])")[, 2:5]
# Check if any matches were found, if not return NA
if (is.na(lat_parts[1, 1])) {
return(NA)
}
degrees <- as.numeric(lat_parts[, 1])
minutes <- as.numeric(lat_parts[, 2])
seconds <- as.numeric(lat_parts[, 3])
hemisphere <- lat_parts[, 4]
# Calculate decimal degrees
decimal_degrees <- degrees + minutes / 60 + seconds / 3600
# Apply positive or negative sign based on hemisphere
ifelse(hemisphere == "N", decimal_degrees, -decimal_degrees)
})
# Converting DMS to decimal degree format. Second the longitude
Finnland2025 <- Finnland2025 %>%
mutate(lng_decimaldegrees = {
# Extract degrees, minutes, seconds, and hemisphere
lng_parts <- str_match(lng_dms, "([0-9.]+)°\\s([0-9.]+)'\\s([0-9.]+)''\\s([EW])")[, 2:5]
# Check if any matches were found, if not return NA
if (is.na(lng_parts[1, 1])) {
return(NA)
}
degrees <- as.numeric(lng_parts[, 1])
minutes <- as.numeric(lng_parts[, 2])
seconds <- as.numeric(lng_parts[, 3])
hemisphere <- lng_parts[, 4]
# Calculate decimal degrees
decimal_degrees <- degrees + minutes / 60 + seconds / 3600
# Apply positive or negative sign based on hemisphere
ifelse(hemisphere == "E", decimal_degrees, -decimal_degrees)
})
head(Finnland2025)
# reorder the columns like this: Date, Time, GPSData, lat_dms, lng_dms, Altitude, lat_decimaldegrees, lng_decimaldegrees, region
# Reorder the columns
Finnland2025 <- Finnland2025[, c("ImageName","Date", "Time", "lat_decimaldegrees", "lng_decimaldegrees", "Altitude", "GPSData", "lat_dms", "lng_dms")]
# arrange entries after Date and time
library(dplyr)
# Assuming your dataframe is named Finnland2025
Finnland2025 <- Finnland2025 %>%
arrange(Date, Time)
Finnland2025
# Load necessary library if not already loaded
library(dplyr)
# Assuming your dataframe is named Finnland2025
Finnland2025 <- Finnland2025 %>%
rename(lat = lat_decimaldegrees)
Finnland2025 <- Finnland2025 %>%
rename(lon = lng_decimaldegrees)
Finnland2025 <- Finnland2025 %>%
rename(elevation = Altitude)
# Add the prefix 'IPhone_Q4_' to each entry in the ID column
Finnland2025$ID <- paste0("IPhone_", Finnland2025$ID)
Finnland2025
library(readxl)
Finnland2025 <- read.csv("/Users/ulrike_imac_air/projects/analysis_my_life/data/photos_gps/2025/2025_Finnland.csv")
# Remove rows where GPSData is NA or empty
Finnland2025 <- Finnland2025[!(is.na(Finnland2025$GPSData) | Finnland2025$GPSData == ""), ]
Finnland2025
# Changing the "Date" from chr to date format
library(dplyr)
Finnland2025 <- Finnland2025 %>%
mutate(Date = as.Date(Date, format = "%Y:%m:%d"))
head(Finnland2025)
# Load necessary library if not already loaded
library(dplyr)
library(chron)
# Assuming your dataframe is named Finnland2025
Finnland2025 <- Finnland2025 %>%
mutate(Time = times(Time))
head(Finnland2025)
# Extracting the first part of GPSData and putting it in a new column called lat_dms
library(dplyr)
library(stringr)
Finnland2025 <- Finnland2025 %>%
mutate(lat_dms = str_extract(GPSData, "^[^,]+"))
# Extracting the second part of GPSData and putting it in a new column called lng_dms
Finnland2025 <- Finnland2025 %>%
mutate(lng_dms = str_extract(GPSData, "(?<=, ).*(?=, )"))
# Extracting the last part of GPSData and putting it in a new column called altitude. And changing data format from chr to integer. Also round values.
Finnland2025 <- Finnland2025 %>%
mutate(Altitude = round(as.numeric(str_extract(GPSData, "(?<=Altitude: )[^,]+")), 2))
head(Finnland2025)
# Converting DMS to decimal degree format. Starting with the latitude
Finnland2025 <- Finnland2025 %>%
mutate(lat_decimaldegrees = {
# Extract degrees, minutes, seconds, and hemisphere
lat_parts <- str_match(lat_dms, "([0-9.]+)°\\s([0-9.]+)'\\s([0-9.]+)''\\s([NS])")[, 2:5]
# Check if any matches were found, if not return NA
if (is.na(lat_parts[1, 1])) {
return(NA)
}
degrees <- as.numeric(lat_parts[, 1])
minutes <- as.numeric(lat_parts[, 2])
seconds <- as.numeric(lat_parts[, 3])
hemisphere <- lat_parts[, 4]
# Calculate decimal degrees
decimal_degrees <- degrees + minutes / 60 + seconds / 3600
# Apply positive or negative sign based on hemisphere
ifelse(hemisphere == "N", decimal_degrees, -decimal_degrees)
})
# Converting DMS to decimal degree format. Second the longitude
Finnland2025 <- Finnland2025 %>%
mutate(lng_decimaldegrees = {
# Extract degrees, minutes, seconds, and hemisphere
lng_parts <- str_match(lng_dms, "([0-9.]+)°\\s([0-9.]+)'\\s([0-9.]+)''\\s([EW])")[, 2:5]
# Check if any matches were found, if not return NA
if (is.na(lng_parts[1, 1])) {
return(NA)
}
degrees <- as.numeric(lng_parts[, 1])
minutes <- as.numeric(lng_parts[, 2])
seconds <- as.numeric(lng_parts[, 3])
hemisphere <- lng_parts[, 4]
# Calculate decimal degrees
decimal_degrees <- degrees + minutes / 60 + seconds / 3600
# Apply positive or negative sign based on hemisphere
ifelse(hemisphere == "E", decimal_degrees, -decimal_degrees)
})
head(Finnland2025)
# reorder the columns like this: Date, Time, GPSData, lat_dms, lng_dms, Altitude, lat_decimaldegrees, lng_decimaldegrees, region
# Reorder the columns
Finnland2025 <- Finnland2025[, c("ImageName","Date", "Time", "lat_decimaldegrees", "lng_decimaldegrees", "Altitude", "GPSData", "lat_dms", "lng_dms")]
# reorder the columns like this: Date, Time, GPSData, lat_dms, lng_dms, Altitude, lat_decimaldegrees, lng_decimaldegrees, region
# Reorder the columns
Finnland2025 <- Finnland2025[, c("ImageName","Date", "Time", "lat_decimaldegrees", "lng_decimaldegrees", "Altitude", "GPSData", "lat_dms", "lng_dms")]
head(Finnland2025)
# arrange entries after Date and time
library(dplyr)
# Assuming your dataframe is named Finnland2025
Finnland2025 <- Finnland2025 %>%
arrange(Date, Time)
Finnland2025
# Load necessary library if not already loaded
library(dplyr)
# Assuming your dataframe is named Finnland2025
Finnland2025 <- Finnland2025 %>%
rename(lat = lat_decimaldegrees)
Finnland2025 <- Finnland2025 %>%
rename(lon = lng_decimaldegrees)
Finnland2025 <- Finnland2025 %>%
rename(elevation = Altitude)
Finnland2025 <- Finnland2025 %>%
rename(ID = ImageName)
Finnland2025
# Add the prefix 'IPhone_Q4_' to each entry in the ID column
Finnland2025$ID <- paste0("IPhone_", Finnland2025$ID)
Finnland2025
# Remove specific columns using subset
Finnland2025 <- subset(Finnland2025, select = -c(GPSData, lat_dms, lng_dms))
Finnland2025
# Extract the year from the Date column and add it as a new column named Year
Finnland2025$Year <- format(Finnland2025$Date, "%Y")
# Reorder the columns
Finnland2025 <- Finnland2025[, c("ID", "Year", "Date", "Time", "lat", "lon", "elevation")]
Finnland2025
# Save the dataframe to a CSV file
write.csv(Finnland2025, file = "Finnland2025_cleaned.csv", row.names = FALSE)
package_list<- c("ggplot2", "dplyr","ineq","WDI", "countrycode", "ggpmisc", "ggrepel")
lapply(package_list, require, character.only = TRUE)
# We are going to download Gini data from the WDI package going back 10 years ago.
# Feel free to edit the data according to your analysis
gini_data <- WDI(country = "all", indicator = "SI.POV.GINI", start = 2015)
package_list<- c("ggplot2", "dplyr","ineq","WDI", "countrycode", "ggpmisc", "ggrepel")
lapply(package_list, require, character.only = TRUE)
#| echo: false
# We are going to download Gini data from the WDI package going back 10 years ago.
# Feel free to edit the data according to your analysis
gini_data <- WDI(country = "all", indicator = "SI.POV.GINI", start = 2015)
install.packages("WDI")
library(WDI)
package_list<- c("ggplot2", "dplyr","ineq","WDI", "countrycode", "ggpmisc", "ggrepel")
lapply(package_list, require, character.only = TRUE)
#| echo: false
# We are going to download Gini data from the WDI package going back 10 years ago.
# Feel free to edit the data according to your analysis
gini_data <- WDI(country = "all", indicator = "SI.POV.GINI", start = 2015)
glimpse(gini_data)
# counting the NAs
# Count total missing values in the whole dataframe
sum(is.na(gini_data))
# Count missing values per variable
colSums(is.na(gini_data))
# you can do the simple calculation above using the sapply function
sapply(gini_data, function(x) mean(is.na(x)) * 100)
# Missing values by country
missing_by_country <- gini_data %>%
group_by(country) %>%
summarise(missing_gini = sum(is.na(SI.POV.GINI)),
total_obs = n()) %>%
arrange(desc(missing_gini))
# Plot missing by country (we will use the top 10 for readability)
ggplot(head(missing_by_country, 25), aes(x = reorder(country, missing_gini), y = missing_gini)) +
geom_col(fill = "#B8907B") +
coord_flip() +
labs(title = "Missing Gini Values by Country (Top 10)",
x = "",
y = "Number of Missing Gini Values")+theme_classic()
# Missing values by country
missing_by_country <- gini_data %>%
group_by(country) %>%
summarise(missing_gini = sum(is.na(SI.POV.GINI)),
total_obs = n()) %>%
arrange(desc(missing_gini))
# Plot missing by country (we will use the top 10 for readability)
ggplot(head(missing_by_country, 50), aes(x = reorder(country, missing_gini), y = missing_gini)) +
geom_col(fill = "#B8907B") +
coord_flip() +
labs(title = "Missing Gini Values by Country (Top 25)",
x = "",
y = "Number of Missing Gini Values")+theme_classic()
# Missing values by country
missing_by_country <- gini_data %>%
group_by(country) %>%
summarise(missing_gini = sum(is.na(SI.POV.GINI)),
total_obs = n()) %>%
arrange(desc(missing_gini))
# Plot missing by country (we will use the top 10 for readability)
ggplot(head(missing_by_country, 20), aes(x = reorder(country, missing_gini), y = missing_gini)) +
geom_col(fill = "#B8907B") +
coord_flip() +
labs(title = "Missing Gini Values by Country (Top 20)",
x = "",
y = "Number of Missing Gini Values")+theme_classic()
# Missing values by year
missing_by_year <- gini_data %>%
group_by(year) %>%
summarise(missing_gini = sum(is.na(SI.POV.GINI)),
total_obs = n())
ggplot(missing_by_year, aes(x = year, y = missing_gini)) +
geom_col(fill = "#648287") +
labs(title = "Missing Gini Values by Year",
x = "Year",
y = "Number of Missing Gini Values") +
scale_x_continuous(breaks = seq(min(missing_by_year$year), max(missing_by_year$year), by = 1)) +
theme_classic()
# mean imputation
gini_data$SI.POV.GINI[is.na(gini_data$SI.POV.GINI)] <-
mean(gini_data$SI.POV.GINI, na.rm = TRUE)
# median imputation
gini_data$SI.POV.GINI[is.na(gini_data$SI.POV.GINI)] <-
median(gini_data$SI.POV.GINI, na.rm = TRUE)
# mode imputation
gini_data$SI.POV.GINI[is.na(gini_data$SI.POV.GINI)] <-
mode(gini_data$SI.POV.GINI, na.rm = TRUE)
# mean imputation
gini_data$SI.POV.GINI[is.na(gini_data$SI.POV.GINI)] <-
mean(gini_data$SI.POV.GINI, na.rm = TRUE)
# median imputation
gini_data$SI.POV.GINI[is.na(gini_data$SI.POV.GINI)] <-
median(gini_data$SI.POV.GINI, na.rm = TRUE)
# Define a function to find the mode (most frequent value)
get_mode <- function(x) {
ux <- unique(x)
tab <- tabulate(match(x, ux))
ux[tab == max(tab)]
}
# Impute missing values with the mode
gini_data$SI.POV.GINI[is.na(gini_data$SI.POV.GINI)] <-
get_mode(gini_data$SI.POV.GINI[!is.na(gini_data$SI.POV.GINI)])
# mean imputation
gini_data$SI.POV.GINI[is.na(gini_data$SI.POV.GINI)] <-
mean(gini_data$SI.POV.GINI, na.rm = TRUE)
# median imputation
gini_data$SI.POV.GINI[is.na(gini_data$SI.POV.GINI)] <-
median(gini_data$SI.POV.GINI, na.rm = TRUE)
# Define a function to find the mode (most frequent value)
get_mode <- function(x) {
ux <- unique(x)
tab <- tabulate(match(x, ux))
ux[tab == max(tab)]
}
# Impute missing values with the mode
gini_data$SI.POV.GINI[is.na(gini_data$SI.POV.GINI)] <-
get_mode(gini_data$SI.POV.GINI[!is.na(gini_data$SI.POV.GINI)])
colSums(is.na(gini_data))’
#mean by country
gini_data_mean <- gini_data %>%
group_by(country) %>%
mutate(SI.POV.GINI = ifelse(is.na(SI.POV.GINI),
mean(SI.POV.GINI, na.rm = TRUE),
SI.POV.GINI)) %>%
ungroup()
# median by country
gini_data_median <- gini_data %>%
group_by(country) %>%
mutate(SI.POV.GINI = ifelse(is.na(SI.POV.GINI),
median(SI.POV.GINI, na.rm = TRUE),
SI.POV.GINI)) %>%
ungroup()
# mode by country
get_mode <- function(x) {
uniq <- na.omit(unique(x))
uniq[which.max(tabulate(match(x, uniq)))]
}
gini_data_mode <- gini_data %>%
group_by(country) %>%
mutate(SI.POV.GINI = ifelse(is.na(SI.POV.GINI),
get_mode(SI.POV.GINI),
SI.POV.GINI)) %>%
ungroup()
# don't forget to verify if the experiment worked well (below)
is.na(gini_data_mean)
is.na(gini_data_median)
is.na(gini_data_mode)
# Verify no missing values remain
colSums(is.na(gini_data_mean))   # or gini_data_median / gini_data_mode
#Check countries with all missing values
gini_data %>%
group_by(country) %>%
summarise(missing_after = sum(is.na(SI.POV.GINI)))
mean(gini_data)
gini_data_mean
gini_data_mean
gini_data_median
gini_data_mode
#| echo: false
# We are going to download Gini data from the WDI package going back 10 years ago.
# Feel free to edit the data according to your analysis
gini_data <- WDI(country = "all", indicator = "SI.POV.GINI", start = 2015)
glimpse(gini_data)
# counting the NAs
# Count total missing values in the whole dataframe
sum(is.na(gini_data))
# Count missing values per variable
colSums(is.na(gini_data))
# mean imputation
gini_data$SI.POV.GINI[is.na(gini_data$SI.POV.GINI)] <-
mean(gini_data$SI.POV.GINI, na.rm = TRUE)
# median imputation
gini_data$SI.POV.GINI[is.na(gini_data$SI.POV.GINI)] <-
median(gini_data$SI.POV.GINI, na.rm = TRUE)
# Define a function to find the mode (most frequent value)
get_mode <- function(x) {
ux <- unique(x)
tab <- tabulate(match(x, ux))
ux[tab == max(tab)]
}
# Impute missing values with the mode
gini_data$SI.POV.GINI[is.na(gini_data$SI.POV.GINI)] <-
get_mode(gini_data$SI.POV.GINI[!is.na(gini_data$SI.POV.GINI)])
# Verify no missing values remain
colSums(is.na(gini_data_mean))   # or gini_data_median / gini_data_mode
#Check countries with all missing values
gini_data %>%
group_by(country) %>%
summarise(missing_after = sum(is.na(SI.POV.GINI)))
gini_data_mean
gini_data_median
gini_data_mode
# Build regression model on non-missing Gini values, using year only
fit <- lm(SI.POV.GINI ~ year, data = gini_data, subset = !is.na(SI.POV.GINI))
# Predict missing values
gini_data$SI.POV.GINI[is.na(gini_data$SI.POV.GINI)] <-
predict(fit, newdata = gini_data[is.na(gini_data$SI.POV.GINI), ])
# Check results
summary(gini_data$SI.POV.GINI)
colSums(is.na(gini_data))  # should now show 0 for SI.POV.GINI
library(gpx)
install.packages("gpx")
library(gpx)
library(sf)
library(dplyr)
library(ggplot2)
library(ggspatial)
install.packages("gpx")
install.packages("ggspatial")
install.packages("patchwork")
trek_data <- read_gpx("/Users/ulrike_imac_air/projects/Trial_and_error/R_code/route_2025-03-20_5.36pm.gpx")
#install.packages("gpx")
#install.packages("ggspatial")
#install.packages("patchwork")
library(gpx)
library(sf)
library(dplyr)
library(ggplot2)
library(ggspatial)
library(patchwork)
trek_data <- read_gpx("/Users/ulrike_imac_air/projects/Trial_and_error/R_code/route_2025-03-20_5.36pm.gpx")
str(trek_data)
trek_tracks <- trek_data[[2]]
str(trek_tracks)
trek <- trek_tracks[[1]]
trek[1:2,]
head(trek[1:2], 2)
str(trek)
files <- list.files(path = "/Users/ulrike_imac_air/projects/Trial_and_error/R_code/Basel",
pattern = "\\.gpx$",
full.names = TRUE)
data_list <- list()
i <- 1
for (file in files) {
data <- read_gpx(file)[[2]][[1]]
data$id = i
i <- i + 1
data_list[[file]] <- data
}
tracks_df <- dplyr::bind_rows(data_list)
str(tracks_df)
treks <- tracks_df |>
st_as_sf(coords = c("Longitude","Latitude"),
crs = 4326)
str(treks)
trek <- treks[treks$id == 1,]
ggplot(data = trek) +
ggspatial::annotation_map_tile(
type = "osm",
cachedir = "maps/",
zoomin = -1) +
geom_sf(aes(color = Elevation)) +
scale_color_viridis_c() +
theme_void() +
labs(
title = paste("Elevation Profile: Park Ridge", as.Date(trek[1,]$Time)),
caption = "Tiles © OpenStreetMap (ODBL license)"
)
install.packages("prettymapr")
library(ggplot2)
library(ggspatial)
ggplot(data = trek) +
ggspatial::annotation_map_tile(
type = "osm",
cachedir = "maps/",
zoomin = -1) +
geom_sf(aes(color = Elevation)) +
scale_color_viridis_c() +
theme_void() +
labs(
title = paste("Elevation Profile: Park Ridge", as.Date(trek[1,]$Time)),
caption = "Tiles © OpenStreetMap (ODBL license)"
)
trek_proj <- trek |> st_transform(crs = 32113)
distance <- trek |>
st_transform(crs = 32113) |>
dplyr::summarise(do_union = F) |>
st_cast("LINESTRING") |>
st_length()
print(paste("Total distance", round(distance/1000, digits = 1), "kilometers"))
data.frame(
"Elevation" = c("Maximum",
"Minimum",
"Average",
"Initial",
"Final"),
"Meters" = c(max(trek$Elevation),
min(trek$Elevation),
round(mean(trek$Elevation)),
trek[1,]$Elevation,
trek[length(trek),]$Elevation)
)
ggplot(trek, aes(x = Time, y = Elevation)) +
geom_line() +
ggtitle(paste("Elevation Profile for Park Ridge Walk on", as.Date(trek[1,]$Time)))
library(patchwork)
gam_plot <- ggplot(trek, aes(x = Time, y = Elevation)) +
geom_line() +
geom_smooth(method = "gam") +
ggtitle(paste("NJ (gam smoothing)", as.Date(trek[1,]$Time)))
loess_plot <- ggplot(trek, aes(x = Time, y = Elevation)) +
geom_line() +
geom_smooth(method = "loess") +
ggtitle(paste("NJ (loess smoothing)", as.Date(trek[1,]$Time)))
rgam_plot | loess_plot
library(patchwork)
rgam_plot <- ggplot(trek, aes(x = Time, y = Elevation)) +
geom_line() +
geom_smooth(method = "gam") +
ggtitle(paste("NJ (gam smoothing)", as.Date(trek[1,]$Time)))
loess_plot <- ggplot(trek, aes(x = Time, y = Elevation)) +
geom_line() +
geom_smooth(method = "loess") +
ggtitle(paste("NJ (loess smoothing)", as.Date(trek[1,]$Time)))
rgam_plot | loess_plot
